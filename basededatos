TRANSFORMACIONES:
map:
from pyspark import SparkContext
sc = SparkContext("local", "Ejemplo de map")
rdd = sc.parallelize([1, 2, 3, 4, 5])
rdd_multiplicado = rdd.map(lambda x: x * 2)
print(rdd_multiplicado.collect())

filter:
from pyspark import SparkContext
sc = SparkContext("local", "Ejemplo de filter")
rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
rdd_pares = rdd.filter(lambda x: x % 2 == 0)
print(rdd_pares.collect())

flatMap:
from pyspark import SparkContext
sc = SparkContext("local", "Ejemplo de flatMap")
rdd = sc.parallelize(["Hola mundo", "Aprendiendo Spark", "flatMap es útil"])
rdd_palabras = rdd.flatMap(lambda frase: frase.split(" "))
print(rdd_palabras.collect())

unión:
from pyspark import SparkContext
sc = SparkContext("local", "Ejemplo de union")
rdd_pares = sc.parallelize([2, 4, 6, 8])
rdd_impares = sc.parallelize([1, 3, 5, 7])
rdd_union = rdd_pares.union(rdd_impares)
print(rdd_union.collect())

intersection:

from pyspark import SparkContext
sc = SparkContext("local", "Ejemplo de intersection")
rdd1 = sc.parallelize([1, 2, 3, 4, 5])
rdd2 = sc.parallelize([3, 4, 5, 6, 7])
rdd_interseccion = rdd1.intersection(rdd2)
print(rdd_interseccion.collect())

distinc:
from pyspark import SparkContext
sc = SparkContext("local", "Ejemplo de distinct")
rdd = sc.parallelize([1, 2, 2, 3, 4, 4, 5, 5, 6, 7, 8, 8, 9])
rdd_distinct = rdd.distinct()
print(rdd_distinct.collect())

groupByKey:
from pyspark import SparkContext
sc = SparkContext("local", "Ejemplo de groupByKey")
rdd = sc.parallelize([
    ("manzana", 3),
    ("manzana", 5),
    ("banana", 4),
    ("manzana", 2),
    ("banana", 1),
    ("cereza", 10)
])
rdd_grouped = rdd.groupByKey()
resultado = rdd_grouped.mapValues(list).collect()
for producto, cantidades in resultado:
    print(f"{producto}: {cantidades}")

reduceByKey:
from pyspark import SparkContext
sc = SparkContext("local", "Ejemplo de reduceByKey")
rdd = sc.parallelize([
    ("manzana", 3),
    ("manzana", 5),
    ("banana", 4),
    ("manzana", 2),
    ("banana", 1),
    ("cereza", 10)
])
rdd_sumado = rdd.reduceByKey(lambda x, y: x + y)
resultado = rdd_sumado.collect()
for producto, total in resultado:
    print(f"{producto}: {total}")

sortByKey:
from pyspark import SparkContext
sc = SparkContext("local", "Ejemplo de sortByKey")
rdd = sc.parallelize([
    (3, "manzana"),
    (1, "banana"),
    (4, "cereza"),
    (2, "durazno")
])
rdd_ordenado = rdd.sortByKey()
resultado = rdd_ordenado.collect()
for clave, valor in resultado:
    print(f"{clave}: {valor}")

join:
from pyspark import SparkContext
sc = SparkContext("local", "Ejemplo de join")
rdd_productos = sc.parallelize([
    (1, "manzana"),
    (2, "banana"),
    (3, "cereza")
])
rdd_precios = sc.parallelize([
    (1, 0.5),
    (2, 0.3),
    (3, 0.7),
    (4, 1.0) 
])
rdd_join = rdd_productos.join(rdd_precios)
resultado = rdd_join.collect()
for clave, (producto, precio) in resultado:
    print(f"ID {clave}: {producto} - ${precio}")

cogroup:
from pyspark import SparkContext
sc = SparkContext("local", "Ejemplo de cogroup")
rdd_productos = sc.parallelize([
    (1, "manzana"),
    (2, "banana"),
    (3, "cereza")
])
rdd_precios = sc.parallelize([
    (1, 0.5),
    (2, 0.3),
    (3, 0.7),
    (1, 0.6) 
])
rdd_cogroup = rdd_productos.cogroup(rdd_precios)
resultado = rdd_cogroup.mapValues(lambda x: (list(x[0]), list(x[1]))).collect()
for clave, (productos, precios) in resultado:
    print(f"ID {clave}: productos={productos}, precios={precios}")

coalesce:
from pyspark import SparkContext
sc = SparkContext("local", "Ejemplo de coalesce")
rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], numSlices=6)
print(f"Número de particiones antes de coalesce: {rdd.getNumPartitions()}")
rdd_reducido = rdd.coalesce(2)
print(f"Número de particiones después de coalesce: {rdd_reducido.getNumPartitions()}")
print(rdd_reducido.collect())

ACCIONES: 
#1 ACCION reduce
rdd = sc.parallelize(range(1, 90), 3)
resultado = rdd.reduce(lambda x, y: x + y)
print(resultado)

#2 ACCION collect
rdd_c = sc.parallelize(["Gnu", "Cat", "Rat", "Dog", "Gnu", "Rat"], 2)
resultado_c = rdd_c.collect()
print(resultado_c)

#3 ACCION count
rdd_a = sc.parallelize(range(1, 5))
resultado_a = rdd_a.count()
print(resultado_a)

#4 ACCION first
rdd_c = sc.parallelize(["Gnu", "Cat", "Rat", "Dog"], 2)
resultado_c_first = rdd_c.first()
print(resultado_c_first)

#5 ACCION take
rdd_b = sc.parallelize(["dog", "cat", "ape", "salmon", "gnu"], 2)
resultado_b_take = rdd_b.take(2)
print(resultado_b_take)

#6 takeSample
rdd_x = sc.parallelize(range(1, 201), 3)
resultado_x_sample = rdd_x.takeSample(True, 20, seed=1)
print(resultado_x_sample)

#7 takeOrdered
rdd_b = sc.parallelize(["dog", "cat", "ape", "salmon", "gnu"], 2)
resultado_b_ordered = rdd_b.takeOrdered(2)
print(resultado_b_ordered)

#8 saveAsTextFile
rdd_a = sc.parallelize(range(1, 10001), 3)
output_path_text = '/content/ejemplo/datos_texto'
rdd_a.saveAsTextFile(output_path_text)

#9 saveAsSequenceFile
rdd_v = sc.parallelize([("owl", 3), ("gnu", 4), ("dog", 1), ("cat", 2), ("ant", 5)], 2)
output_path_seq = '/content/ejemplo/seq_datos'
rdd_v.saveAsSequenceFile(output_path_seq)

#10 saveAsObjectFile
rdd_x = sc.parallelize([("owl", 3), ("gnu", 4), ("dog", 1), ("cat", 2), ("ant", 5)], 2)
output_path_obj = "/content/ejemplo/objFile"
rdd_x.saveAsObjectFile(output_path_obj)

#11 countByKey
rdd_values_key = sc.parallelize([("apple", 1), ("banana", 1), ("apple", 1), ("orange", 1), ("banana", 1), ("apple", 1)], 2)
resultado_countByKey = rdd_values_key.countByKey()
print(dict(resultado_countByKey))

#12 ACCION foreach
rdd_c = sc.parallelize(["cat", "dog", "tiger", "lion", "gnu", "crocodile", "ant", "whale", "dolphin", "spider"], 3)
resultado_foreach = rdd_c.map(lambda x: f"{x}s are beautiful").collect()
for line in resultado_foreach:
    print(line)
